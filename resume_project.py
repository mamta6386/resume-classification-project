# -*- coding: utf-8 -*-
"""Resume_project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11v-sI91TD9RUkuYwWC30hbbEL8kH8kWj
"""

import os
import re
import nltk
import spacy
import string
import pandas as pd
import numpy as np
import seaborn as sns
from matplotlib import pylab
from matplotlib import pyplot as plt

from nltk.corpus import stopwords
from nltk.tokenize import RegexpTokenizer
from wordcloud import WordCloud, STOPWORDS
from nltk.stem import PorterStemmer, WordNetLemmatizer
from sklearn.feature_extraction.text import CountVectorizer


import warnings
warnings.filterwarnings('ignore')

from google.colab import drive
drive.mount('/content/drive')

os.listdir('/content/drive/MyDrive/Resumes')

pip install textract

!sudo apt-get install -y antiword

import textract

file_path1 = []
category1  = []
directory1 = '/content/drive/MyDrive/Resumes/Peoplesoft resumes'
for i in os.listdir(directory1):
    if i.endswith('.docx') or i.endswith('.doc') or i.endswith('.pdf'):
        os.path.join(directory1, i)
        file_path1.append((textract.process(os.path.join(directory1, i))).decode('utf-8'))
        category1.append('PeopleSoft')

data1 = pd.DataFrame(data = file_path1 , columns = ['Raw_Details'])
data1['Category1'] = category1
data1

file_path2 = []
category2  = []
directory2 = '/content/drive/MyDrive/Resumes/React Developer'
for i in os.listdir(directory2):
    if i.endswith('.docx') or i.endswith('.doc') or i.endswith('.pdf'):
        os.path.join(directory2, i)
        file_path2.append((textract.process(os.path.join(directory2, i))).decode('utf-8'))
        category2.append('React JS Developer')

data2 = pd.DataFrame(data = file_path2 , columns = ['Raw_Details'])
data2['Category2'] = category2
data2

file_path3 = []
category3  = []
directory3 = '/content/drive/MyDrive/Resumes/SQL Developer Lightning insight'
for i in os.listdir(directory3):
   if i.endswith('.docx') or i.endswith('.doc') or i.endswith('.pdf'):
        os.path.join(directory3, i)
        file_path3.append((textract.process(os.path.join(directory3, i))).decode('utf-8'))
        category3.append('SQL Developer')

data3 = pd.DataFrame(data = file_path3 , columns = ['Raw_Details'])
data3['Category3'] = category3
data3

file_path4 = []
category4  = []
directory4 = '/content/drive/MyDrive/Resumes/workday resumes'
for i in os.listdir(directory4):
   if i.endswith('.docx') or i.endswith('.doc') or i.endswith('.pdf'):
        os.path.join(directory4, i)
        file_path4.append((textract.process(os.path.join(directory4, i))).decode('utf-8'))
        category4.append('Workday')

data4 = pd.DataFrame(data = file_path4 , columns = ['Raw_Details'])
data4['Category4'] = category4
data4

Resume_data = data1.append([data2, data3, data4], ignore_index = True)
Resume_data

Resume_data.info()

Resume_data['Category'] = category1 + category2 + category3 + category4
Resume_data

Resume_data.drop(['Category1', 'Category2', 'Category3', 'Category4'], axis = 1, inplace = True)
resume_data = Resume_data[["Category", "Raw_Details"]]

Resume_data.head(15)

Resume_data.info()

Resume_data.describe()

Resume_data.isnull().sum()

Resume_data.shape

Resume_data["Raw_Details"][1]

Resume_data["Raw_Details"][2]

Resume_data["Raw_Details"][4]

Resume_data["Raw_Details"][9]

Resume_data.to_csv('Raw_Resume.csv', index=False)

Resume_data = pd.read_csv("Raw_Resume.csv")
Resume_data

Resume_data

import nltk
nltk.download('stopwords')

import pandas as pd
import re

# Assuming 'Raw_Details' is a column in the DataFrame 'Resume_data'
# You can replace 'Raw_Details' with the actual column name in your dataset
Resume_data['Raw_Details'] = Resume_data['Raw_Details'].apply(lambda x: re.sub(r'[^\w\s]', '', x))

# Display the modified dataset
print(Resume_data)

resume_data[resume_data.Category == 'Workday']

resume_data[resume_data.Category == 'PeopleSoft']

resume_data[resume_data.Category == 'React JS Developer']

resume_data[resume_data.Category == 'SQL Developer']

from sklearn.preprocessing import LabelEncoder
le_encoder = LabelEncoder()
Resume_data["Encoded_Skill"] = le_encoder.fit_transform(Resume_data["Category"])
Resume_data.head()

Resume_data.Category.value_counts()

def preprocess(sentence):
    sentence = str(sentence)
    sentence = sentence.lower()
    sentence = sentence.replace('{html}',"")
    cleanr = re.compile('<.*?>')
    cleantext = re.sub(cleanr, '', sentence)
    rem_url = re.sub(r'http\S+', '',cleantext)
    rem_num = re.sub(r'\b[0-9]+\b', '', rem_url)
    tokenizer = RegexpTokenizer(r'\w+')
    tokens = tokenizer.tokenize(rem_num)
    filtered_words = [w for w in tokens if len(w) > 2 if not w in stopwords.words('english')]

    return " ".join(filtered_words)

Resume_data

frequency = pd.Series(' '.join(Resume_data['Raw_Details']).split()).value_counts()[:20] #For top 20
frequency

from nltk.corpus import stopwords
stop = stopwords.words('english')
Resume_data['Raw_Details'] = Resume_data['Raw_Details'].apply(lambda x: " ".join(x for x in x.split() if x not in stop))

frequency_new = pd.Series(' '.join(Resume_data['Raw_Details']).split()).value_counts()

frequency_new

frequency_new = pd.Series(' '.join(Resume_data['Raw_Details']).split()).value_counts()[:20] # for top 20
frequency_new

filtered_frequency = frequency_new[frequency_new <= 20]

filtered_frequency

top_20_filtered_frequency = filtered_frequency[:20]
print(top_20_filtered_frequency)

Resume_data

Resume_data_1 = Resume_data.to_csv('Cleaned_Resumes.csv', index = False)

Resume_data_1 = pd.read_csv('Cleaned_Resumes.csv')
Resume_data_1

import nltk
nltk.download('punkt')

"""Named Entity Recognition (NER)


"""

oneSetOfStopWords = set(stopwords.words('english')+['``',"''"])
totalWords =[]
Sentences = resume_data['Raw_Details'].values
cleanedSentences = ""
for records in Sentences:
    cleanedText = preprocess(records)
    cleanedSentences += cleanedText
    requiredWords = nltk.word_tokenize(cleanedText)
    for word in requiredWords:
        if word not in oneSetOfStopWords and word not in string.punctuation:
            totalWords.append(word)

wordfreqdist = nltk.FreqDist(totalWords)
mostcommon = wordfreqdist.most_common(50)
print(mostcommon)

import spacy

# Load the spaCy English model
nlp = spacy.load("en_core_web_sm")


# Process the text with spaCy
doc = nlp("Resume_data")

# Extract and print POS tags for each word
for token in doc:
    print(f"{token.text}: {token.pos_}")
    # Extract and print fine-grained POS tags for each word
for token in doc:
    print(f"{token.text}: {token.tag_}")

    one_block = cleanedSentences[1300:5200]
doc_block = nlp(one_block)
spacy.displacy.render(doc_block, style= 'ent', jupyter= True)

import spacy
from spacy import displacy

text=nlp(Resume_data["Raw_Details"][0])
displacy.render(text, style = "ent", jupyter = True)

for token in doc_block[:30]:
    print(token,token.pos_)

one_block = cleanedSentences
doc_block = nlp(one_block)
nouns_verbs = [token.text for token in doc_block if token.pos_ in ('NOUN','VERB')]
print(nouns_verbs[:250])

cv = CountVectorizer()
X = cv.fit_transform(nouns_verbs)
sum_words = X.sum(axis=0)

words_freq = [(word,sum_words[0,idx]) for word, idx in cv.vocabulary_.items()]
words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)

wd_df = pd.DataFrame(words_freq)
wd_df.columns = ['Words','Count']
wd_df[0:15]

characters=Resume_data["Raw_Details"].apply(len)
characters

fig, axe = plt.subplots(1,1, figsize=(10,7), dpi=200)
ax = sns.barplot(x= wd_df['Count'].head(20), y= wd_df.Words.head(20), data= wd_df, ax = axe,
            label= 'Total Pofile Category : {}'.format(len(resume_data.Category.unique())))

axe.set_xlabel('Frequency', size=16,fontweight= 'bold')
axe.set_ylabel('Words', size=16, fontweight= 'bold')
plt.xticks(rotation = 0)
plt.legend(loc='best', fontsize= 'x-large')
plt.title('Top 25 Most used Nouns and Verbs in Resumes', fontsize= 18, fontweight= 'bold')
rcParams = {'xtick.labelsize':'14','ytick.labelsize':'14','axes.labelsize':'16'}

for i in ax.containers:
    ax.bar_label(i,color = 'black', fontweight = 'bold', fontsize= 12)

pylab.rcParams.update(rcParams)
fig.tight_layout()
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt
sns.distplot(x = characters)

from collections import Counter
import seaborn as sns

words =['using','Workday','Experience','PeopleSoft',
 'experience','SQL','Application','data','Server',
 'business','Project','reports','like','HCM','Worked',
 'knowledge','Involved','various','Good', 'Reports','React','EIB','integrations','Web','system','creating','issues',
 'Created', 'Responsibilities','Process','process','support',
 'application','new','People','I','team','working',
 'Database','database','Integration','Domains','client',
 'requirements','Core',  'Business',
'Oracle','Report', 'Developer', 'Data']
indices = np.random.zipf(1.6, size=500).astype(np.int) % len(words)
tw = np.array(words)[indices]

tf = Counter(tw)

y = [count for tag, count in tf.most_common(50)]
x = [tag for tag, count in tf.most_common(50)]
plt.style.use('seaborn-dark-palette')
plt.figure(figsize=(12,5))
plt.bar(x, y, color=['gold','lightcoral', 'lightskyblue'])
plt.title("Word frequencies in Resume Data in Log Scale")
plt.ylabel("Frequency (log scale)")
plt.yscale('symlog') # optionally set a log scale for the y-axis
plt.xticks(rotation=90)
for i, (tag, count) in enumerate(tf.most_common(50)):
    plt.text(i, count, f' {count} ', rotation=90,
             ha='center', va='top' if i < 10 else 'bottom', color='white' if i < 10 else 'black')
plt.xlim(-0.6, len(x)-0.4) # optionally set tighter x lims
plt.tight_layout() # change the whitespace such that all labels fit nicely
plt.show()

def wordBarGraphFunction_1(df,column,title):
    topic_words = [ z.lower() for y in
                       [ x.split() for x in df[column] if isinstance(x, str)]
                       for z in y]
    word_count_dict = dict(Counter(topic_words))
    popular_words = sorted(word_count_dict, key = word_count_dict.get, reverse = True)
    popular_words_nonstop = [w for w in popular_words if w not in stopwords.words("english")]
    plt.style.use('fivethirtyeight')
    sns.barplot(x=np.arange(20),y= [word_count_dict[w] for w in reversed(popular_words_nonstop[0:20])])
    plt.xticks([x + 0.5 for x in range(20)], reversed(popular_words_nonstop[0:20]),rotation=90)
    plt.title(title)
    plt.show()

plt.figure(figsize=(15,6))
wordBarGraphFunction_1(Resume_data,"Raw_Details","Most frequent Words ")

import seaborn as sns
import matplotlib.pyplot as plt
plt.style.use('seaborn-dark-palette')
plt.figure(figsize=(15,7))
plt.title("The distinct categories of resumes")
plt.xticks(rotation=90)
sns.countplot(y="Category", data=Resume_data,palette=("Set2"))
plt.show()

# Commented out IPython magic to ensure Python compatibility.
# Import packages
import matplotlib.pyplot as plt
# %matplotlib inline
from wordcloud import WordCloud, STOPWORDS
# Define a function to plot word cloud
def plot_cloud(wordcloud):
    # Set figure size
    plt.figure(figsize=(40, 30))
    # Display image
    plt.imshow(wordcloud)
    # No axis details
    plt.axis("off");

# Generate wordcloud
stopwords = STOPWORDS
stopwords.add('will')
wordcloud = WordCloud(width = 3000, height = 2000, background_color='black', max_words=100,colormap='Set2',stopwords=stopwords).generate(str(Resume_data))
# Plot
plot_cloud(wordcloud)

text = " ".join(cat for cat in wd_df.Words) # Creating the text variable

word_cloud = WordCloud(width=1000, height=800, random_state=10, background_color="black",
                       colormap="Pastel1", collocations=False, stopwords=STOPWORDS).generate(text)

plt.figure(figsize=(10,7), dpi=800) # Display the generated Word Cloud
plt.title('Most used Nouns and Verbs in Resumes', fontsize= 15, fontweight= 'bold')
plt.imshow(word_cloud)
plt.axis("off")

plt.show()

"""BAG OF WORDS"""

from sklearn.feature_extraction.text import CountVectorizer
import pandas as pd

requiredText = Resume_data["Raw_Details"]
requiredTarget = Resume_data["Encoded_Skill"].values

# Create a CountVectorizer instance
Countvectorizer = CountVectorizer(analyzer='word', token_pattern=r'\b[^\d\s]+\b', stop_words='english')

# Fit and transform the text data
bag = Countvectorizer.fit_transform(requiredText)

# Get the vocabulary (word to index mapping)
vocabulary = Countvectorizer.vocabulary_

# Convert the vocabulary to a DataFrame and transpose it
df_vocabulary = pd.DataFrame(list(vocabulary.items()), columns=['Word', 'Index']).set_index('Word').T

# Display the transposed vocabulary
print("Vocabulary:")
print(df_vocabulary)

requiredText = Resume_data["Raw_Details"]
requiredTarget = Resume_data["Encoded_Skill"].values
Countvectorizer=CountVectorizer(analyzer='word',token_pattern=r'\b[^\d\s]+\b',stop_words = 'english')
bag = Countvectorizer.fit_transform(requiredText)
vocabulary =Countvectorizer.vocabulary_

from sklearn.feature_extraction.text import CountVectorizer
import pandas as pd

# Sample data (replace this with your actual data)
requiredText = Resume_data["Raw_Details"]
requiredTarget = Resume_data["Encoded_Skill"].values

# Create a CountVectorizer instance
Countvectorizer = CountVectorizer(analyzer='word', token_pattern=r'\b[^\d\s]+\b', stop_words='english')

# Fit and transform the text data
bag = Countvectorizer.fit_transform(requiredText)

# Get the feature names
feature_names = Countvectorizer.get_feature_names_out()

# Convert the bag-of-words matrix to a DataFrame
df_bag = pd.DataFrame(bag.toarray(), columns=feature_names)

# Display the DataFrame
print("Bag-of-Words Representation:")
print(df_bag)

from sklearn.feature_extraction.text import CountVectorizer
import pandas as pd
import numpy as np

vectorizer1 = CountVectorizer(min_df=1, max_df=0.9)
count_vect = vectorizer1.fit_transform(vocabulary)
word_freq_df = pd.DataFrame({'term': vectorizer1.get_feature_names_out(), 'occurrences': np.asarray(count_vect.sum(axis=0)).ravel().tolist()})
word_freq_df['frequency'] = word_freq_df['occurrences'] / np.sum(word_freq_df['occurrences'])

# Display the DataFrame
print(word_freq_df)

vectorizer1 = CountVectorizer(min_df = 1, max_df = 0.9)
count_vect = vectorizer1.fit_transform(vocabulary)
word_freq_df = pd.DataFrame({'term': vectorizer1.get_feature_names_out(), 'occurrences':np.asarray(count_vect.sum(axis=0)).ravel().tolist()})
word_freq_df['frequency'] = word_freq_df['occurrences']/np.sum(word_freq_df['occurrences'])
word_freq_df

"""N- GRAM"""

pip install textblob

from textblob import TextBlob
TextBlob(resume_data['Raw_Details'][1]).ngrams(1)[:20]

TextBlob(resume_data['Raw_Details'][1]).ngrams(2)[:20]

TextBlob(resume_data['Raw_Details'][1]).ngrams(3)[:20]

Resume_data['Raw_Details']

"""TFIDF - Term frequency inverse Document Frequency"""

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from scipy.sparse import hstack
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

word_vectorizer = TfidfVectorizer(sublinear_tf=True, stop_words='english')
word_vectorizer.fit(requiredText)
WordFeatures = word_vectorizer.transform(requiredText)

WordFeatures

"""Model Building || Model Training || Model Evaluation"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(WordFeatures, requiredTarget, random_state=42, test_size=0.3, stratify=requiredTarget)

X_train

X_test

X_train.shape

X_test.shape

y_train.shape

y_test.shape

y_train

y_test

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import seaborn as sns
sns.set_style('darkgrid')
# %matplotlib inline
import matplotlib.pyplot as plt
import matplotlib.pylab as pylab
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer

from sklearn.svm import SVC
from xgboost import XGBClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.ensemble import BaggingClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import confusion_matrix, accuracy_score
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import f1_score, classification_report, precision_score, recall_score

"""1. LOGISTIC REGRESSION"""

model_lgr = LogisticRegression()
model_lgr.fit(X_train, y_train)
y_pred = model_lgr.predict(X_test)
accuracy_lgr = accuracy_score(y_test, y_pred)
print('Accuracy of training set : {:.2f}'.format(model_lgr.score(X_train, y_train)))
print('Accuracy of  test set    : {:.2f}'.format(model_lgr.score(X_test, y_test)))
print("Classification report for classifier %s:\n%s\n" % (model_lgr,classification_report(y_test, y_pred)))
nb_score = model_lgr.score(X_test, y_test)
nb_cm = confusion_matrix(y_test, y_pred)


precision_lgr = round(precision_score(y_test,y_pred,average = 'macro'),2)
recall_lgr = round(recall_score(y_test,y_pred, average = 'macro'),2)
f1_lgr = round(f1_score(y_test,y_pred, average = 'macro'),2)
accuracy_lgr = round(accuracy_score(y_test,y_pred),2)

"""2. DECISION TREE"""

from sklearn import tree
from sklearn.tree import DecisionTreeClassifier

model_DT = DecisionTreeClassifier(criterion='gini')
model_DT.fit(X_train, y_train)
y_pred = model_DT.predict(X_test)
accuracy_DT = accuracy_score(y_test, y_pred)
print('Accuracy of training set : {:.2f}'.format(model_DT.score(X_train, y_train)))
print('Accuracy of  test set    : {:.2f}'.format(model_DT.score(X_test, y_test)))
print("Classification report for classifier %s:\n%s\n" % (model_DT,classification_report(y_test, y_pred)))
nb_score = model_DT.score(X_test, y_test)
nb_cm = confusion_matrix(y_test, y_pred)

precision_DT = round(precision_score(y_test,y_pred,average = 'macro'),2)
recall_DT= round(recall_score(y_test,y_pred, average = 'macro'),2)
f1_DT = round(f1_score(y_test,y_pred, average = 'macro'),2)
accuracy_DT = round(accuracy_score(y_test,y_pred),2)

"""3. RANDOM FOREST"""

from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier

model_RF = RandomForestClassifier(n_estimators=200)
model_RF.fit(X_train, y_train)
y_pred = model_RF.predict(X_test)
accuracy_RF = accuracy_score(y_test, y_pred)
print('Accuracy of training set : {:.2f}'.format(model_RF.score(X_train, y_train)))
print('Accuracy of  test set    : {:.2f}'.format(model_RF.score(X_test, y_test)))
print("Classification report for classifier %s:\n%s\n" % (model_RF,classification_report(y_test, y_pred)))
nb_score = model_RF.score(X_test, y_test)
nb_cm = confusion_matrix(y_test, y_pred)


precision_RF = round(precision_score(y_test,y_pred,average = 'macro'),2)
recall_RF = round(recall_score(y_test,y_pred, average = 'macro'),2)
f1_RF = round(f1_score(y_test,y_pred, average = 'macro'),2)
accuracy_RF = round(accuracy_score(y_test,y_pred),2)

"""4.NAIVE BAYES"""

from sklearn.naive_bayes import MultinomialNB
model = MultinomialNB()

model_NB =MultinomialNB(alpha=1, fit_prior=False, class_prior=None)
model_NB.fit(X_train, y_train)
y_pred = model_NB.predict(X_test)
accuracy_NB = accuracy_score(y_test, y_pred)
print('Accuracy of training set : {:.2f}'.format(model_NB.score(X_train, y_train)))
print('Accuracy of  test set    : {:.2f}'.format(model_NB.score(X_test, y_test)))
print("Classification report for classifier %s:\n%s\n" % (model_NB,classification_report(y_test, y_pred)))
nb_score = model_NB.score(X_test, y_test)
nb_cm = confusion_matrix(y_test, y_pred)

precision_NB = round(precision_score(y_test,y_pred,average = 'macro'),2)
recall_NB = round(recall_score(y_test,y_pred, average = 'macro'),2)
f1_NB = round(f1_score(y_test,y_pred, average = 'macro'),2)
accuracy_NB = round(accuracy_score(y_test,y_pred),2)

"""5. SUPPORT VECTOR MACHINE"""

from sklearn import svm
from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV

model_svm = SVC()
model_svm.fit(X_train, y_train)
y_pred = model_svm.predict(X_test)
accuracy_svm = accuracy_score(y_test, y_pred)
print('Accuracy of training set : {:.2f}'.format(model_svm.score(X_train, y_train)))
print('Accuracy of  test set    : {:.2f}'.format(model_svm.score(X_test, y_test)))
print("Classification report for classifier %s:\n%s\n" % (model_svm,classification_report(y_test, y_pred)))
nb_score = model_svm.score(X_test, y_test)
nb_cm = confusion_matrix(y_test, y_pred)

precision_svm = round(precision_score(y_test,y_pred,average = 'macro'),2)
recall_svm = round(recall_score(y_test,y_pred, average = 'macro'),2)
f1_svm = round(f1_score(y_test,y_pred, average = 'macro'),2)
accuracy_svm = round(accuracy_score(y_test,y_pred),2)

"""6.KNN Classifier"""

from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

model_knn = KNeighborsClassifier(n_neighbors=41)
model_knn.fit(X_train, y_train)
y_pred = model_knn.predict(X_test)
accuracy_knn = accuracy_score(y_test, y_pred)
print('Accuracy of training set : {:.2f}'.format(model_knn.score(X_train, y_train)))
print('Accuracy of  test set    : {:.2f}'.format(model_knn.score(X_test, y_test)))
print("Classification report for classifier %s:\n%s\n" % (model_knn,classification_report(y_test, y_pred)))
nb_score = model_knn.score(X_test, y_test)
nb_cm = confusion_matrix(y_test, y_pred)

precision_knn = round(precision_score(y_test,y_pred,average = 'macro'),2)
recall_knn = round(recall_score(y_test,y_pred, average = 'macro'),2)
f1_knn = round(f1_score(y_test,y_pred, average = 'macro'),2)
accuracy_knn = round(accuracy_score(y_test,y_pred),2)

""" 7. Bagging Classifier"""

from sklearn.model_selection import train_test_split
from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

model_bagg = BaggingClassifier(KNeighborsClassifier(), max_samples=0.5, max_features=0.5)
model_bagg.fit(X_train, y_train)
y_pred = model_bagg.predict(X_test)
accuracy_bagg = accuracy_score(y_test, y_pred)
print('Accuracy of training set : {:.2f}'.format(model_bagg.score(X_train, y_train)))
print('Accuracy of  test set    : {:.2f}'.format(model_bagg.score(X_test, y_test)))
print("Classification report for classifier %s:\n%s\n" % (model_bagg,classification_report(y_test, y_pred)))
nb_score = model_bagg.score(X_test, y_test)
nb_cm = confusion_matrix(y_test, y_pred)

precision_bagg = round(precision_score(y_test,y_pred,average = 'macro'),2)
recall_bagg = round(recall_score(y_test,y_pred, average = 'macro'),2)
f1_bagg = round(f1_score(y_test,y_pred, average = 'macro'),2)
accuracy_bagg = round(accuracy_score(y_test,y_pred),2)

""" 8. Gradient Boosting Classifier"""

model_GradientBoost = GradientBoostingClassifier(n_estimators=100,learning_rate=1.0,max_depth=1, random_state=0)
model_GradientBoost.fit(X_train, y_train)
y_pred = model_GradientBoost.predict(X_test)
accuracy_GradientBoost = accuracy_score(y_test, y_pred)
print('Accuracy of training set : {:.2f}'.format(model_GradientBoost.score(X_train, y_train)))
print('Accuracy of  test set    : {:.2f}'.format(model_GradientBoost.score(X_test, y_test)))
print("Classification report for classifier %s:\n%s\n" % (model_GradientBoost,classification_report(y_test, y_pred)))
nb_score = model_GradientBoost.score(X_test, y_test)
nb_cm = confusion_matrix(y_test, y_pred)

precision_Gradientboost = round(precision_score(y_test,y_pred,average = 'macro'),2)
recall_Gradientboost = round(recall_score(y_test,y_pred, average = 'macro'),2)
f1_Gradientboost = round(f1_score(y_test,y_pred, average = 'macro'),2)
accuracy_Gradientboost = round(accuracy_score(y_test,y_pred),2)

model_Adaboost = AdaBoostClassifier(n_estimators=100)
model_Adaboost.fit(X_train, y_train)
y_pred = model_Adaboost.predict(X_test)
accuracy_Adaboost = accuracy_score(y_test, y_pred)
print('Accuracy of training set : {:.2f}'.format(model_Adaboost.score(X_train, y_train)))
print('Accuracy of  test set    : {:.2f}'.format(model_Adaboost.score(X_test, y_test)))
print("Classification report for classifier %s:\n%s\n" % (model_Adaboost,classification_report(y_test, y_pred)))
nb_score = model_Adaboost.score(X_test, y_test)
nb_cm = confusion_matrix(y_test, y_pred)

precision_Adaboost = round(precision_score(y_test,y_pred,average = 'macro'),2)
recall_Adaboost = round(recall_score(y_test,y_pred, average = 'macro'),2)
f1_Adaboost = round(f1_score(y_test,y_pred, average = 'macro'),2)
accuracy_Adaboost = round(accuracy_score(y_test,y_pred),2)

Evaluation = {"Models":["KNN Classifier","DecisionTree Classifier","RandomForest Classifier","SVM Classifier",
                        "Logistic Regression","Bagging Classifier","AdaBoost Classifier","Gradient Boosting Classifier","Naive Bayes Classifier"],\
            "Train_Accuracy(%)":[model_knn.score(X_train, y_train),model_DT.score(X_train, y_train),model_RF.score(X_train, y_train),model_svm.score(X_train, y_train),model_lgr.score(X_train, y_train),
                                 model_bagg.score(X_train, y_train),model_Adaboost.score(X_train, y_train),model_GradientBoost.score(X_train, y_train), model_NB.score(X_train, y_train)],
            "Test_Accuracy(%)":[accuracy_knn,accuracy_DT,accuracy_RF,accuracy_svm,accuracy_lgr,accuracy_bagg,accuracy_Adaboost,accuracy_GradientBoost,accuracy_NB],\
            "Precision(%)":[precision_knn,precision_DT,precision_RF,precision_svm,precision_lgr,precision_bagg,precision_Adaboost,precision_Gradientboost,precision_NB],\
            "Recall(%)":[recall_knn,recall_DT,recall_RF,recall_svm,recall_lgr,recall_bagg,recall_Adaboost,recall_Gradientboost,recall_NB],\
            "F1-Score(%)":[f1_knn,f1_DT,f1_RF,f1_svm,f1_lgr,f1_bagg,f1_Adaboost,f1_Gradientboost,f1_NB]}
table = pd.DataFrame(Evaluation)
table

import matplotlib.pyplot as plt

fig, ax = plt.subplots(figsize=(20,5))
fig.suptitle('Model Performance')

ax.plot(table['Models'], table['Train_Accuracy(%)'], marker='o')
ax.plot(table['Models'], table['Test_Accuracy(%)'], marker='o')
ax.plot(table['Models'], table['Precision(%)'], marker='s')
ax.plot(table['Models'], table['Recall(%)'], marker='^')
ax.plot(table['Models'], table['F1-Score(%)'], marker='d')
ax.set_ylabel('Score')
ax.legend(['Training Accuracy', 'Testing Accuracy', 'Precision', 'Recall', 'F1 Score'], loc='lower right')

plt.xticks(rotation=90)

plt.show()

import matplotlib.pyplot as plt
import numpy as np

fig, ax = plt.subplots(figsize=(20,6))
fig.suptitle('Model Performance')

metrics = ['Train_Accuracy(%)', 'Test_Accuracy(%)', 'Precision(%)', 'Recall(%)', 'F1-Score(%)']
# Plot bars for each classifier and each metric
bottom = np.zeros(len(table))
for metric in metrics:
    ax.bar(table['Models'], table[metric], bottom=bottom)
    bottom += table[metric]
# Set axis labels and legend
ax.set_ylabel('Score')
ax.legend(metrics, loc='lower right')
plt.xticks(rotation=90)
plt.show()

fig, ax = plt.subplots(figsize=(20, 7))
fig.suptitle('Models Performance')

metrics = ['Train Accuracy(%)', 'Test Accuracy(%)', 'Precision(%)', 'Recall(%)', 'F1-Score(%)']

bar_width = 0.1
opacity = 0.8
index = np.arange(len(table['Models']))

# Plot bars for each classifier and each metric
rects1 = ax.bar(index, table['Train_Accuracy(%)'], bar_width, alpha=opacity, color='b', label='Training Accuracy')
rects2 = ax.bar(index + bar_width, table['Test_Accuracy(%)'], bar_width, alpha=opacity, color='g', label='Testing Accuracy')
rects3 = ax.bar(index + 2*bar_width, table['Precision(%)'], bar_width, alpha=opacity, color='r', label='Precision')
rects4 = ax.bar(index + 3*bar_width, table['Recall(%)'], bar_width, alpha=opacity, color='y', label='Recall')
rects5 = ax.bar(index + 4*bar_width, table['F1-Score(%)'], bar_width, alpha=opacity, color='m', label='F1 Score')

# Set axis labels and legend
ax.set_xlabel('Models')
ax.set_ylabel('Score')
ax.set_xticks(index + 2.5*bar_width)
ax.set_xticklabels(table['Models'], rotation=90)
ax.legend()

plt.show()

"""MODEL DEPLOYMENT"""

import pickle
from pickle import dump
filename = 'modelNB.pkl'
pickle.dump(model_NB,open(filename,'wb'))

import pickle
from pickle import dump
filename =  'wordvectorizer.pkl'
pickle.dump(word_vectorizer,open(filename,'wb'))

from google.colab import files
files.download('modelNB.pkl')
files.download('wordvectorizer.pkl')

